name: Continuous Deployment

on:
  push:
    branches:
      - main

jobs:
  build-and-push-image:
    runs-on: ubuntu-24.04
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and push Airflow image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: vunewbie/airflow:1.0.0
          cache-from: type=registry,ref=vunewbie/airflow:1.0.0
          cache-to: type=inline

  upload-pyspark-to-gcs:
    runs-on: ubuntu-24.04
    needs: build-and-push-image
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY_JSON }}

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v2

      - name: Build aw.zip for PySpark dependencies
        run: |
          rm -f pyspark/elt/libs/aw.zip
          (cd pyspark/elt/libs && zip -r aw.zip aw)

      - name: Upload pyspark folder to GCS
        run: |
          gsutil -m rsync -r -d pyspark/ gs://from-warehouse-to-lakehouse-bucket/pyspark

      - name: Verify upload
        run: |
          gsutil ls gs://from-warehouse-to-lakehouse-bucket/pyspark/elt/workflows/source_to_landing.py
          gsutil ls gs://from-warehouse-to-lakehouse-bucket/pyspark/elt/libs/aw.zip

  deploy-to-vm:
    runs-on: ubuntu-24.04
    needs:
      - build-and-push-image
      - upload-pyspark-to-gcs
    steps:
      - name: Deploy via SSH to VM
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.SSH_HOST }}
          username: ${{ secrets.SSH_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          port: ${{ secrets.SSH_PORT || '22' }}
          script: |
            set -e  # Exit on error
            cd ~/from-warehouse-to-lakehouse
            
            # Stop containers before git pull to avoid permission issues
            echo "Stopping containers before git pull..."
            docker compose down || true

            # Sleep for 10 seconds to avoid permission issues
            echo "Sleeping for 10 seconds..."
            sleep 10
            
            # Pull latest code
            echo "Pulling latest code..."
            git pull origin main
            
            # Pull latest images
            echo "Pulling latest images..."
            docker compose pull
            
            # Start containers
            echo "Starting containers..."
            docker compose up -d
            
            # Wait for services to be healthy with retry logic
            echo "Waiting for Airflow webserver to be ready..."
            MAX_RETRIES=30
            RETRY_INTERVAL=10
            RETRY_COUNT=0
            
            while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
              if curl -f http://localhost:8080/health > /dev/null 2>&1; then
                echo "Health check passed!"
                echo "Deployment successful!"
                exit 0
              fi
              
              RETRY_COUNT=$((RETRY_COUNT + 1))
              echo "Health check attempt $RETRY_COUNT/$MAX_RETRIES failed, retrying in ${RETRY_INTERVAL}s..."
              sleep $RETRY_INTERVAL
            done
            
            # If we reach here, health check failed after all retries
            echo "Health check failed after $MAX_RETRIES attempts!"
            echo "Checking container status..."
            docker compose ps
            echo "Checking webserver logs..."
            docker compose logs --tail=50 airflow-webserver
            exit 1