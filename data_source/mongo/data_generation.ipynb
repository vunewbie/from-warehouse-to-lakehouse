{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60bbe0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6731adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geo_json(row):\n",
    "    \"\"\"\n",
    "    Creates a nested geolocation dictionary from a DataFrame row.\n",
    "    Returns None if the latitude is NaN (indicating no merge match).\n",
    "    \"\"\"\n",
    "    if pd.isna(row['geolocation_lat']): \n",
    "        return None\n",
    "    return {\n",
    "        \"geolocation_zip_code_prefix\": row['geolocation_zip_code_prefix'],\n",
    "        \"geolocation_lat\": row['geolocation_lat'],\n",
    "        \"geolocation_lng\": row['geolocation_lng'],\n",
    "        \"geolocation_city\": row['geolocation_city'],\n",
    "        \"geolocation_state\": row['geolocation_state']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6140a81",
   "metadata": {},
   "source": [
    "# 1. geolocation collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6f1f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocation = pd.read_csv('olist_geolocation_dataset.csv')\n",
    "\n",
    "# Prepare the standalone geolocation collection for MongoDB\n",
    "geolocation_collection = geolocation.to_dict(orient='records')\n",
    "\n",
    "# Remove duplicates to ensure unique combination of zip, city, and state for merging to customers and sellers\n",
    "geolocation = geolocation.drop_duplicates(subset=[\n",
    "    'geolocation_zip_code_prefix', \n",
    "    'geolocation_city', \n",
    "    'geolocation_state'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6225ed90",
   "metadata": {},
   "source": [
    "# 2. customer collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cdf1e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_csv('olist_customers_dataset.csv')\n",
    "\n",
    "# Merge customers with geolocation data based on Zip, City, and State\n",
    "# Using 'left' join ensures customers are kept even if no geo match is found (simulating dirty data)\n",
    "merged_customers_geo = customers.merge(\n",
    "    geolocation, \n",
    "    left_on=['customer_zip_code_prefix', 'customer_city', 'customer_state'], \n",
    "    right_on=['geolocation_zip_code_prefix', 'geolocation_city', 'geolocation_state'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Create nested 'geolocation' field\n",
    "merged_customers_geo['geolocation'] = merged_customers_geo.apply(create_geo_json, axis=1)\n",
    "\n",
    "# Drop flat geolocation columns to keep the document structure clean\n",
    "cols_to_drop = [\n",
    "    'geolocation_lat', 'geolocation_lng', \n",
    "    'geolocation_city', 'geolocation_state', 'geolocation_zip_code_prefix'\n",
    "]\n",
    "merged_customers_geo = merged_customers_geo.drop(columns=cols_to_drop)\n",
    "\n",
    "# Convert to list of dictionaries\n",
    "customers_collection = merged_customers_geo.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84976cf",
   "metadata": {},
   "source": [
    "# 3. sellers collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e34e21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers = pd.read_csv('olist_sellers_dataset.csv')\n",
    "\n",
    "# Merge sellers with geolocation data based on Zip, City, and State\n",
    "merged_sellers_geo = sellers.merge(\n",
    "    geolocation, \n",
    "    left_on=['seller_zip_code_prefix', 'seller_city', 'seller_state'], \n",
    "    right_on=['geolocation_zip_code_prefix', 'geolocation_city', 'geolocation_state'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Create nested 'geolocation' field\n",
    "merged_sellers_geo['geolocation'] = merged_sellers_geo.apply(create_geo_json, axis=1)\n",
    "\n",
    "# Drop flat geolocation columns\n",
    "merged_sellers_geo = merged_sellers_geo.drop(columns=cols_to_drop)\n",
    "\n",
    "# Convert to list of dictionaries\n",
    "sellers_collection = merged_sellers_geo.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62a240a",
   "metadata": {},
   "source": [
    "# 4. product collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0863084",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('olist_products_dataset.csv')\n",
    "product_category_translation = pd.read_csv('product_category_name_translation.csv')\n",
    "\n",
    "# Normalize for better matching\n",
    "products['product_category_name'] = products['product_category_name'].str.lower().str.strip()\n",
    "product_category_translation['product_category_name'] = product_category_translation['product_category_name'].str.lower().str.strip()\n",
    "\n",
    "# Merge products with translations to get English category names\n",
    "merged_products = products.merge(product_category_translation, on='product_category_name', how='left')\n",
    "\n",
    "# Convert to list of dictionaries\n",
    "products_collection = merged_products.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eec34f",
   "metadata": {},
   "source": [
    "# 5. order collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f663227c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vuloc\\AppData\\Local\\Temp\\ipykernel_17896\\203589426.py:9: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  items_grp = items_df.groupby('order_id').apply(\n",
      "C:\\Users\\vuloc\\AppData\\Local\\Temp\\ipykernel_17896\\203589426.py:14: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  payments_grp = payments_df.groupby('order_id').apply(\n",
      "C:\\Users\\vuloc\\AppData\\Local\\Temp\\ipykernel_17896\\203589426.py:19: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  reviews_grp = reviews_df.groupby('order_id').apply(\n"
     ]
    }
   ],
   "source": [
    "orders_df = pd.read_csv('olist_orders_dataset.csv')\n",
    "items_df = pd.read_csv('olist_order_items_dataset.csv')\n",
    "payments_df = pd.read_csv('olist_order_payments_dataset.csv')\n",
    "reviews_df = pd.read_csv('olist_order_reviews_dataset.csv')\n",
    "\n",
    "# --- Grouping Items ---\n",
    "# Group items by order_id and convert to list of dictionaries\n",
    "# Drop 'order_id' from the nested objects to avoid redundancy\n",
    "items_grp = items_df.groupby('order_id').apply(\n",
    "    lambda x: x.drop('order_id', axis=1).to_dict('records')\n",
    ").reset_index(name='order_items')\n",
    "\n",
    "# --- Grouping Payments ---\n",
    "payments_grp = payments_df.groupby('order_id').apply(\n",
    "    lambda x: x.drop('order_id', axis=1).to_dict('records')\n",
    ").reset_index(name='order_payments')\n",
    "\n",
    "# --- Grouping Reviews ---\n",
    "reviews_grp = reviews_df.groupby('order_id').apply(\n",
    "    lambda x: x.drop('order_id', axis=1).to_dict('records')\n",
    ").reset_index(name='order_reviews')\n",
    "\n",
    "# --- Merging into Main Orders DataFrame ---\n",
    "# Left join to ensure all orders are kept, even if they have no items/payments/reviews\n",
    "orders_final = orders_df.merge(items_grp, on='order_id', how='left')\n",
    "orders_final = orders_final.merge(payments_grp, on='order_id', how='left')\n",
    "orders_final = orders_final.merge(reviews_grp, on='order_id', how='left')\n",
    "\n",
    "# --- Handling Missing Nested Data ---\n",
    "# If an order has no items/payments, the merge results in NaN. \n",
    "# We replace NaN with empty lists [] for consistency in MongoDB.\n",
    "for col in ['order_items', 'order_payments', 'order_reviews']:\n",
    "    orders_final[col] = orders_final[col].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# --- Converting Date Columns ---\n",
    "# Convert string dates to Python datetime objects for proper storage in MongoDB\n",
    "date_cols = [\n",
    "    'order_purchase_timestamp', 'order_approved_at', \n",
    "    'order_delivered_carrier_date', 'order_delivered_customer_date', \n",
    "    'order_estimated_delivery_date'\n",
    "]\n",
    "for col in date_cols:\n",
    "    orders_final[col] = pd.to_datetime(orders_final[col])\n",
    "    orders_final[col] = orders_final[col].astype(object).where(orders_final[col].notnull(), None)\n",
    "\n",
    "# Convert to list of dictionaries\n",
    "orders_collection = orders_final.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa3eab",
   "metadata": {},
   "source": [
    "# 6. insert to mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00511daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoClient(host=['ac-mh9db4u-shard-00-02.a04msnj.mongodb.net:27017', 'ac-mh9db4u-shard-00-00.a04msnj.mongodb.net:27017', 'ac-mh9db4u-shard-00-01.a04msnj.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, appname='Cluster0', authsource='admin', replicaset='atlas-7va5io-shard-0', tls=True)\n"
     ]
    }
   ],
   "source": [
    "# Connect to local MongoDB instance\n",
    "# Replace connection string if using a cloud database (e.g., MongoDB Atlas)\n",
    "client = MongoClient('')\n",
    "\n",
    "# Create (or access) the database for the Legacy System\n",
    "db = client['olist_legacy_system']\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64f1a307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully migrated Olist dataset to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "# Clear existing data to avoid duplicates during testing (Optional)\n",
    "db.geolocation.drop()\n",
    "db.customers.drop()\n",
    "db.sellers.drop()\n",
    "db.products.drop()\n",
    "db.orders.drop()\n",
    "\n",
    "# Insert data into collections\n",
    "# Using insert_many for batch insertion which is faster\n",
    "db.geolocation.insert_many(geolocation_collection)\n",
    "db.customers.insert_many(customers_collection)\n",
    "db.sellers.insert_many(sellers_collection)\n",
    "db.products.insert_many(products_collection)\n",
    "db.orders.insert_many(orders_collection)\n",
    "\n",
    "# Verification message\n",
    "print(\"Successfully migrated Olist dataset to MongoDB!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
